# üé∏ Detector de Acordes - Proyecto ML

Sistema de detecci√≥n autom√°tica de acordes en m√∫sica usando t√©cnicas de Machine Learning. Este proyecto implementa dos modelos principales: un baseline MLP que trabaja con segmentos de audio agrupados por beats, y un modelo basado en frames que procesa el audio frame por frame con mayor resoluci√≥n temporal.

## üìã Tabla de Contenidos

- [Requerimientos](#requerimientos)
- [Instalaci√≥n](#instalaci√≥n)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Modelo Baseline MLP](#modelo-baseline-mlp)
  - [Crear Dataset](#crear-dataset-baseline-mlp)
  - [Entrenar Modelo](#entrenar-modelo-baseline-mlp)
  - [Inferencia](#inferencia-baseline-mlp)
  - [Evaluaci√≥n](#evaluaci√≥n-baseline-mlp)
- [Modelo de Frames](#modelo-de-frames)
  - [Crear Dataset](#crear-dataset-frames)
  - [Entrenar Modelo](#entrenar-modelo-frames)
  - [Inferencia](#inferencia-frames)
  - [Evaluaci√≥n](#evaluaci√≥n-frames)
- [Interfaz Web](#interfaz-web)
- [Estructura de Datos](#estructura-de-datos)

## üîß Requerimientos

### Dependencias Python

El proyecto requiere Python 3.9+ y las siguientes librer√≠as principales:

- **TensorFlow/Keras**: Para los modelos de redes neuronales
- **librosa**: Para procesamiento de audio y extracci√≥n de features
- **scikit-learn**: Para preprocesamiento y evaluaci√≥n
- **pandas/numpy**: Para manipulaci√≥n de datos
- **matplotlib**: Para visualizaciones
- **Flask**: Para la interfaz web

### Instalaci√≥n de Dependencias

```bash
# Crear entorno virtual (recomendado)
python -m venv env

# Activar entorno virtual
# En Mac/Linux:
source env/bin/activate
# En Windows:
env\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### Datos Requeridos

El proyecto espera tener los datos de The Beatles Annotations en la siguiente estructura:

```
The Beatles Annotations/
‚îú‚îÄ‚îÄ audio/
‚îÇ   ‚îî‚îÄ‚îÄ The Beatles/
‚îÇ       ‚îî‚îÄ‚îÄ [√°lbumes y canciones .mp3]
‚îú‚îÄ‚îÄ chordlab/
‚îÇ   ‚îî‚îÄ‚îÄ The Beatles/
‚îÇ       ‚îî‚îÄ‚îÄ [archivos .lab con anotaciones de acordes]
‚îî‚îÄ‚îÄ ...
```

## üìÅ Estructura del Proyecto

```
chord_detection/
‚îú‚îÄ‚îÄ The Beatles Annotations/     # Dataset de audio y anotaciones
‚îú‚îÄ‚îÄ analysis_out/                # Resultados del modelo baseline MLP
‚îú‚îÄ‚îÄ analysis_out_frames/         # Resultados del modelo de frames
‚îú‚îÄ‚îÄ outputs/                     # Archivos .lab generados por inferencia
‚îú‚îÄ‚îÄ baseline_mlp.py              # Script de entrenamiento baseline MLP
‚îú‚îÄ‚îÄ build_dataset.py             # Generaci√≥n de dataset para baseline MLP
‚îú‚îÄ‚îÄ build_frames_dataset.py      # Generaci√≥n de dataset para modelo frames
‚îú‚îÄ‚îÄ train_dnn_frames.py          # Script de entrenamiento modelo frames
‚îú‚îÄ‚îÄ inference_baseline_mlp.py    # Script de inferencia baseline MLP
‚îú‚îÄ‚îÄ inference_frames.py          # Script de inferencia modelo frames
‚îú‚îÄ‚îÄ evaluate_wcsr.py            # Script de evaluaci√≥n WCSR
‚îú‚îÄ‚îÄ train_test_split.json        # Divisi√≥n train/test por canci√≥n
‚îú‚îÄ‚îÄ dataset_chords.csv           # Dataset generado para baseline MLP
‚îú‚îÄ‚îÄ frames_dataset.csv           # Dataset generado para modelo frames
‚îú‚îÄ‚îÄ frames_dataset.npz           # Dataset binario para modelo frames
‚îú‚îÄ‚îÄ pca.joblib                   # Modelo PCA para frames
‚îú‚îÄ‚îÄ scaler.joblib                # Scaler para frames
‚îú‚îÄ‚îÄ web/                         # Interfaz web Flask
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îî‚îÄ‚îÄ requirements.txt
```

## üéØ Modelo Baseline MLP

El modelo baseline MLP trabaja con segmentos de audio agrupados por beats. Cada segmento representa varios beats (por defecto 4) y se caracteriza por un vector de cromas agregado.

### Crear Dataset (Baseline MLP)

El script `build_dataset.py` procesa los archivos de audio y genera un CSV con features de cromas agregadas por segmento de acorde.

```bash
python build_dataset.py
```

**Salida:**
- `dataset_chords.csv`: Dataset con columnas:
  - `album_track`: Identificador de la canci√≥n
  - `t_start`, `t_end`: Tiempos de inicio y fin del segmento
  - `label`: Etiqueta del acorde (formato `Root:maj` o `Root:min`)
  - `chroma_C`, `chroma_Db`, ..., `chroma_B`: Valores de cromas (12 dimensiones)

**Caracter√≠sticas:**
- Usa chroma CQT est√°ndar de 12 bins (una por nota)
- Agrega cromas por mediana dentro de cada intervalo de acorde
- Filtra segmentos con label "N" (sin acorde)
- Normaliza cromas por columna

### Entrenar Modelo (Baseline MLP)

```bash
python baseline_mlp.py
```

**Proceso:**
1. Carga el dataset desde `dataset_chords.csv`
2. Divide train/test usando `train_test_split.json` (por canci√≥n)
3. Divide train en train/validation (80/20 por canci√≥n)
4. Estandariza features con `StandardScaler`
5. Entrena dos arquitecturas:
   - **MLP Original**: 2 capas densas (128 unidades cada una)
   - **MLP Bottleneck**: Arquitectura con compresi√≥n-expansi√≥n (128‚Üí64‚Üí32‚Üí64‚Üí128)
6. Selecciona el mejor modelo seg√∫n accuracy en test
7. Genera m√©tricas y visualizaciones

**Salidas en `analysis_out/`:**
- `baseline_mlp_model.h5`: Modelo entrenado (mejor arquitectura)
- `mlp_label_mapping.txt`: Mapeo de √≠ndices a nombres de clases
- `mlp_scaler_stats.npz`: Estad√≠sticas del scaler (media y desviaci√≥n est√°ndar)
- `baseline_mlp_confusion.png`: Matriz de confusi√≥n
- `baseline_mlp_loss.png`: Curvas de p√©rdida
- `baseline_mlp_predictions.csv`: Predicciones en test
- `best_model_report_*.txt`: Reporte de clasificaci√≥n del mejor modelo

**Par√°metros configurables en el script:**
- `BATCH = 64`: Tama√±o de batch
- `EPOCHS = 100`: N√∫mero m√°ximo de √©pocas
- `DROPOUT = 0.40`: Tasa de dropout
- `TEST_SIZE = 0.25`: Proporci√≥n de test

### Inferencia (Baseline MLP)

```bash
python inference_baseline_mlp.py <ruta_al_audio> [opciones]
```

**Opciones principales:**
- `--model`: Ruta al modelo `.h5` (default: `analysis_out/baseline_mlp_model.h5`)
- `--labels`: Ruta al mapeo de labels (default: `analysis_out/mlp_label_mapping.txt`)
- `--scaler`: Ruta al scaler stats (default: `analysis_out/mlp_scaler_stats.npz`)
- `--beats-per-segment`: N√∫mero de beats por segmento (default: 4)
- `--use-hmm`: Usar HMM para suavizar predicciones
- `--hmm`: Ruta al modelo HMM (default: `analysis_out/hmm_model.npz`)
- `--transition-weight`: Peso de transiciones HMM (default: 0.3)
- `--out`: Prefijo de salida (default: `outputs/<nombre_audio>`)

**Ejemplo:**
```bash
python inference_baseline_mlp.py test_audios/cancion.mp3 --beats-per-segment 4 --use-hmm
```

**Salida:**
- Archivo `.lab` en `outputs/` con predicciones en formato:
  ```
  t_start    t_end      label_pred
  0.000000   2.500000   C:maj
  2.500000   5.000000   G:maj
  ...
  ```

### Evaluaci√≥n (Baseline MLP)

```bash
python evaluate_wcsr.py baseline_mlp <split> [opciones]
```

**Ejemplo:**
```bash
# Evaluar en test split
python evaluate_wcsr.py baseline_mlp test

# Evaluar en train split con HMM
python evaluate_wcsr.py baseline_mlp train --use-hmm-baseline --beats-per-segment 4

# Limitar cantidad de canciones para prueba r√°pida
python evaluate_wcsr.py baseline_mlp test --max-songs 10
```

**Opciones:**
- `split`: `train` o `test`
- `--beats-per-segment`: N√∫mero de beats por segmento (default: 4)
- `--use-hmm-baseline`: Usar HMM para suavizar
- `--transition-weight`: Peso de transiciones HMM (default: 0.3)
- `--max-songs`: L√≠mite de canciones a evaluar

**Salidas:**
- CSV con resultados por canci√≥n en `analysis_out/wcsr_*_beats_results.csv`
- Archivos `.lab` predichos en `outputs/` o `outputs_test/`
- M√©tricas: WCSR global, promedio, mediana, desviaci√≥n est√°ndar

## üéº Modelo de Frames

El modelo de frames procesa el audio frame por frame con mayor resoluci√≥n temporal. Usa CQT (Constant-Q Transform) con splicing de contexto temporal.

### Crear Dataset (Frames)

```bash
python build_frames_dataset.py
```

**Proceso:**
1. Procesa cada archivo de audio frame por frame
2. Calcula CQT log-magnitude (180 bins, 5 octavas)
3. Aplica PCA (retenci√≥n 98% de varianza) solo en train
4. Estandariza con `StandardScaler` solo en train
5. Aplica splicing de contexto (t-1, t, t+1) ‚Üí 3 frames concatenados
6. Genera labels por frame desde anotaciones `.lab`

**Salidas:**
- `frames_dataset.csv`: Metadatos (album_track, tiempo, label)
- `frames_dataset.npz`: Dataset binario con:
  - `X`: Features (N, F') donde F' = (2*ctx+1) * PCA_dims
  - `y`: Labels (N,)
  - `groups`: Identificadores de canci√≥n
  - `times`: Tiempos de cada frame
- `pca.joblib`: Modelo PCA entrenado
- `scaler.joblib`: Scaler entrenado
- `analysis_out_frames/class_balance_counts_frames.csv`: Distribuci√≥n de clases

**Caracter√≠sticas:**
- Sample rate: 11025 Hz
- Hop length: 512 (~46.4 ms por frame)
- CQT: 180 bins (36 bins/octava √ó 5 octavas)
- Contexto: ¬±1 frame (SPLICE_CTX=1)
- Filtra frames con label "N"

### Entrenar Modelo (Frames)

```bash
python train_dnn_frames.py
```

**Proceso:**
1. Carga dataset desde `frames_dataset.npz`
2. Divide train/test por canci√≥n (75/25)
3. Divide train en train/validation (80/20 por canci√≥n)
4. Entrena dos arquitecturas:
   - **MLP Com√∫n**: 2 capas densas (1024 unidades cada una)
   - **MLP Bottleneck**: Arquitectura con compresi√≥n-expansi√≥n (1024‚Üí512‚Üí256‚Üí512‚Üí1024)
5. Usa class weights balanceados
6. Selecciona mejor modelo seg√∫n accuracy

**Salidas en `analysis_out_frames/`:**
- `dnn_common.h5`: Modelo MLP com√∫n
- `dnn_bottleneck.h5`: Modelo MLP bottleneck
- `label_mapping.txt`: Mapeo de √≠ndices a nombres de clases
- `class_weights.txt`: Pesos de clases balanceados
- `train_val_loss.png`: Curvas de p√©rdida comparativas
- `report_common.txt`: Reporte de clasificaci√≥n (com√∫n)
- `report_bottleneck.txt`: Reporte de clasificaci√≥n (bottleneck)

**Par√°metros configurables:**
- `BATCH = 128`: Tama√±o de batch
- `EPOCHS = 50`: N√∫mero m√°ximo de √©pocas
- Dropout: 0.3 en todas las capas

### Inferencia (Frames)

```bash
python inference_frames.py <ruta_al_audio> [opciones]
```

**Opciones:**
- `--smooth`: Usar HMM para suavizar predicciones (default: activado)
- `--beat-sync`: Alinear predicciones a beats y usar majority voting
- `--beat-group`: Cantidad de beats por grupo para voting (default: 2)

**Ejemplo:**
```bash
# Inferencia b√°sica con HMM
python inference_frames.py test_audios/cancion.mp3 --smooth

# Inferencia con beat-sync
python inference_frames.py test_audios/cancion.mp3 --smooth --beat-sync --beat-group 4

# Sin HMM
python inference_frames.py test_audios/cancion.mp3
```

**Rutas por defecto (configurables en el script):**
- `PCA = Path("pca.joblib")`
- `SCAL = Path("scaler.joblib")`
- `MODEL = Path("analysis_out_frames/dnn_bottleneck.h5")`
- `MAP = Path("analysis_out_frames/label_mapping.txt")`

**Salida:**
- Archivo `.lab` en `outputs/` con predicciones frame por frame fusionadas

### Evaluaci√≥n (Frames)

```bash
python evaluate_wcsr.py frames <split> [opciones]
```

**Ejemplo:**
```bash
# Evaluar en test split
python evaluate_wcsr.py frames test

# Evaluar sin HMM
python evaluate_wcsr.py frames test --no-hmm-frames

# Evaluar con beat-sync
python evaluate_wcsr.py frames_beatsync test --beat-group 4
```

**Opciones:**
- `split`: `train` o `test`
- `--use-hmm-frames`: Usar HMM (default: True)
- `--no-hmm-frames`: Desactivar HMM
- `--beat-group`: Para `frames_beatsync`, cantidad de beats por grupo

**Salidas:**
- CSV con resultados en `analysis_out_frames/wcsr_*_results.csv`
- Archivos `.lab` predichos en `outputs/` o `outputs_test/`
- M√©tricas: WCSR promedio, mediana, desviaci√≥n est√°ndar

## üåê Interfaz Web

La interfaz web permite subir archivos de audio y visualizar los acordes detectados en tiempo real.

### Instalaci√≥n de Dependencias Web

```bash
cd web
pip install -r requirements.txt
```

### Ejecutar Servidor Web

**Opci√≥n 1: Scripts de inicio (recomendado)**

En Mac/Linux:
```bash
cd web
./start.sh
```

En Windows:
```bash
cd web
start.bat
```

**Opci√≥n 2: Manual**
```bash
cd web
python app.py
```

El servidor se iniciar√° en `http://localhost:5000`

### Uso de la Interfaz Web

1. **Abrir navegador**: Navega a `http://localhost:5000`
2. **Subir audio**: Arrastra y suelta un archivo o haz clic para seleccionar
   - Formatos soportados: MP3, WAV, OGG, M4A, FLAC
   - Tama√±o m√°ximo: 50MB
3. **Procesamiento**: El modelo procesar√° el audio autom√°ticamente
4. **Visualizaci√≥n**: Los acordes se mostrar√°n sincronizados con el reproductor

**Caracter√≠sticas:**
- Reproductor de audio integrado
- Visualizaci√≥n de acordes en tiempo real
- L√≠nea de tiempo interactiva
- Click en acorde para saltar a ese momento

**Nota**: La interfaz web usa el modelo de frames (`inference_frames.py`) con HMM y beat-sync activados por defecto.

## üìä Estructura de Datos

### Formato de Archivos .lab

Los archivos `.lab` contienen anotaciones de acordes en formato:

```
t_start    t_end      label
0.000000   2.500000   C:maj
2.500000   5.000000   G:maj
5.000000   7.500000   A:min
...
```

- **t_start, t_end**: Tiempos en segundos (formato float)
- **label**: Etiqueta del acorde en formato `Root:maj` o `Root:min`
  - Ra√≠ces: C, Db, D, Eb, E, F, Gb, G, Ab, A, Bb, B
  - Calidades: `maj` (mayor) o `min` (menor)

### Formato train_test_split.json

```json
{
  "train_songs": [
    "album1/track1",
    "album1/track2",
    ...
  ],
  "test_songs": [
    "album2/track1",
    "album2/track2",
    ...
  ]
}
```

Este archivo define la divisi√≥n train/test por canci√≥n para evitar data leakage.

## üîç M√©tricas de Evaluaci√≥n

### WCSR (Weighted Chord Symbol Recall)

El WCSR mide la proporci√≥n de tiempo donde las etiquetas predichas coinciden con las de referencia, excluyendo segmentos marcados como "N" (sin acorde).

- **WCSR Global**: Ponderado por duraci√≥n total
- **WCSR Promedio**: Promedio aritm√©tico por canci√≥n
- **WCSR Mediana**: Mediana de WCSR por canci√≥n

## üìù Notas Adicionales

- Los modelos se entrenan con **class weights balanceados** para manejar desbalance de clases
- Se usa **early stopping** y **reducci√≥n de learning rate** durante el entrenamiento
- El **HMM** se puede usar para suavizar predicciones y mejorar coherencia temporal
- El modelo de frames tiene mayor resoluci√≥n temporal pero requiere m√°s recursos computacionales
- El modelo baseline MLP es m√°s r√°pido pero con menor resoluci√≥n temporal

## üêõ Soluci√≥n de Problemas

### Error: "No se encontr√≥ el modelo"
Aseg√∫rate de haber entrenado el modelo correspondiente antes de ejecutar inferencia o evaluaci√≥n.

### Error: "No module named 'tensorflow'"
Instala las dependencias: `pip install -r requirements.txt`

### Error: "FileNotFoundError: dataset_chords.csv"
Ejecuta primero `build_dataset.py` para generar el dataset.

### El puerto 5000 est√° ocupado (web)
Edita `web/app.py` y cambia el puerto:
```python
app.run(debug=True, port=8000)  # Usar puerto 8000
```

## üìö Referencias

- [librosa](https://librosa.org/) - Procesamiento de audio
- [TensorFlow/Keras](https://www.tensorflow.org/) - Deep Learning
- [The Beatles Annotations Dataset](https://github.com/tmc323/Chord-Annotations)

---

**Autor**: Proyecto de Machine Learning - Detecci√≥n de Acordes  
**Fecha**: 2025

